# [A Short History of RAG](https://cobusgreyling.medium.com/a-short-history-of-rag-d2935a2fea29)

![RAG](./rag.jpg)

## イントロ

現在、大規模言語モデルに関する最も人気のあるテーマの一つは、「検索強化生成（Retrieval Augmented Generation, RAG）」のアイデアです。RAGはいくつかの理由で重要ですが、この記事ではまず、RAGの人気が高まった原因について議論します。そして、最も基本的な形でのRAGの説明を行います。

## なぜRAGが必要か

### エマージェント(創発)機能

少し前から、LLM（大規模言語モデル）は「創発機能」と呼ばれるものを持っていると見なされていました。創発機能とは、モデルがユーザーやタスクとのやりとりの中で示す予期せぬ、または以前には予見されなかった機能のことです。

LLMにおける創発機能には次のようなものがあります：

- **問題解決**：LLMは、言語の理解と推理能力を活かし、明示的に訓練されていないタスクに対して洞察に満ちた解決策やアプローチを提供することで、問題解決能力を示すことがあります。
- **ユーザー入力への適応**：LLMは、特定のユーザー入力や文脈に応じてその反応を適応させ、やり取りのダイナミクスに基づいて出力にパーソナライゼーションやカスタマイズを示します。
- **文脈理解**：LLMは文脈の繊細な理解を示し、プロンプトに明示的に記述されていない場合でも、与えられた文脈に適切かつ適切な反応を生成することができます。

創発機能の現象は、組織がこれまで知られていなかったLLMの機能を発見しようとする動きにつながりました。

**しかし、創発機能という概念は幻想であることが証明されました。**

最近の研究では、創発機能という考え方が幻想であるとされ、実際には、一般的にLLMが文脈を与えられたときに非常にうまく応答するという事実が、創発能力と見なされていたことがわかりました。

### 指示と文脈参照

推論時に指示に基づいてLLMはうまく反応します。また、プロンプトに文脈参照データが注入されたとき、LLMは非常にうまく反応することがわかっています。多くの研究によって、推論時に提供される文脈知識が、モデルがファインチューニングされたデータよりも優先されることが実証的に証明されています。

### 文脈内学習
文脈内学習は、ユーザーや現在のタスクから提供される特定の文脈に基づいて、モデルがその反応を適応させて洗練する能力を指します。

このプロセスにより、モデルは操作している文脈を考慮して、より関連性が高く、一貫性のある出力を生成することが可能になります。

![In context learning](./incontextlearning.jpg)

### 勾配を用いないアプローチ

LLMをプログラミングする方法には二つのアプローチがあります。LLMをプログラミングする別の方法として、データをLLMにどのように提供するかという視点があります。

データの提供は、勾配を用いるか勾配を用いないかのどちらかです。

![LLM Programming](./llmprograming.jpg)

